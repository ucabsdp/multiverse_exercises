{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 - Transfer Learning VGG16 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.preprocessing.image import img_to_array, array_to_img\n",
    "import tensorflow.image as tf_image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in data, we will limit the dataset sizes for examples sake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 1000\n",
    "TEST_SIZE = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG16(weights='imagenet', include_top=False, input_shape=(48, 48, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess to rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(images):\n",
    "    \"\"\" Preprocess grey scale image to rgb and resizes\n",
    "    \n",
    "    Args:\n",
    "        images (np.array): Array of grey scale image\n",
    "        \n",
    "    Returns:\n",
    "        np.array: Array of converted images\n",
    "    \"\"\"\n",
    "    image_rgb = np.dstack((images, np.zeros_like(images), np.zeros_like(images)))\n",
    "    image_rgb = image_rgb.reshape(image_rgb.shape[0], 28, 28, 3)\n",
    "    image_rgb = np.asarray([img_to_array(array_to_img(im, scale=False).resize((48,48))) for im in image_rgb])\n",
    "    return image_rgb\n",
    "    \n",
    "x_train_rgb = preprocess(x_train)\n",
    "y_train_oh = to_categorical(y_train)\n",
    "\n",
    "x_test_rgb = preprocess(x_test)\n",
    "y_test_oh = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract features from VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1, 1, 512)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_rgb.shape\n",
    "train_features = model.predict(x_train_rgb[:TRAIN_SIZE])\n",
    "train_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train simple mlp on the extracted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 512)\n",
      "Epoch 1/20\n",
      "1000/1000 [==============================] - 1s 834us/step - loss: 1.5923 - accuracy: 0.5220\n",
      "Epoch 2/20\n",
      "1000/1000 [==============================] - 0s 126us/step - loss: 0.7892 - accuracy: 0.7750\n",
      "Epoch 3/20\n",
      "1000/1000 [==============================] - 0s 130us/step - loss: 0.5353 - accuracy: 0.8510\n",
      "Epoch 4/20\n",
      "1000/1000 [==============================] - 0s 120us/step - loss: 0.4116 - accuracy: 0.8850\n",
      "Epoch 5/20\n",
      "1000/1000 [==============================] - 0s 121us/step - loss: 0.3170 - accuracy: 0.9140\n",
      "Epoch 6/20\n",
      "1000/1000 [==============================] - 0s 122us/step - loss: 0.2256 - accuracy: 0.9550\n",
      "Epoch 7/20\n",
      "1000/1000 [==============================] - 0s 121us/step - loss: 0.1717 - accuracy: 0.9710\n",
      "Epoch 8/20\n",
      "1000/1000 [==============================] - 0s 129us/step - loss: 0.1344 - accuracy: 0.9840\n",
      "Epoch 9/20\n",
      "1000/1000 [==============================] - 0s 122us/step - loss: 0.0978 - accuracy: 0.9890\n",
      "Epoch 10/20\n",
      "1000/1000 [==============================] - 0s 127us/step - loss: 0.0761 - accuracy: 0.9940\n",
      "Epoch 11/20\n",
      "1000/1000 [==============================] - 0s 121us/step - loss: 0.0582 - accuracy: 0.9950\n",
      "Epoch 12/20\n",
      "1000/1000 [==============================] - 0s 120us/step - loss: 0.0458 - accuracy: 0.9970\n",
      "Epoch 13/20\n",
      "1000/1000 [==============================] - 0s 120us/step - loss: 0.0373 - accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "1000/1000 [==============================] - 0s 121us/step - loss: 0.0302 - accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "1000/1000 [==============================] - 0s 121us/step - loss: 0.0261 - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "1000/1000 [==============================] - 0s 121us/step - loss: 0.0228 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "1000/1000 [==============================] - 0s 122us/step - loss: 0.0193 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "1000/1000 [==============================] - 0s 123us/step - loss: 0.0172 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "1000/1000 [==============================] - 0s 129us/step - loss: 0.0153 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "1000/1000 [==============================] - 0s 121us/step - loss: 0.0134 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x8a8a54f98>"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Dense, Activation\n",
    "from keras.models import Model\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "\n",
    "shape = np.array(train_features.shape)\n",
    "train_features = np.reshape(train_features, (shape[0], shape[-1]))\n",
    "print(train_features.shape)\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(512, activation='relu', input_dim=512))\n",
    "model.add(Dense(500, activation='sigmoid'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "\n",
    "# fitting the model \n",
    "\n",
    "model.fit(train_features, y_train_oh[:TRAIN_SIZE], epochs=20, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
